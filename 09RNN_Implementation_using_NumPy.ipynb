{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnwarMirza/SEP740-Deep-Learning-McMaster-Summer2023/blob/main/09RNN_Implementation_using_NumPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiK3V-cELDCf"
      },
      "source": [
        "# RNN Implementation using NumPy\n",
        "\n",
        "- Sequence input, Char-level, Batch training, Python 3\n",
        "\n",
        "- You can train RNNs using different vector representations. (Try \"glove\" or \"word2vec\" to train the sequence of words instead of the \"One-hot-vector\")\n",
        "\n",
        "- This is a slightly modified version of the original code(Karpathy).\n",
        "\n",
        "URL\n",
        "\n",
        "-> https://github.com/JY-Yoon\n",
        "\n",
        "-> Original code(Karpathy) : https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PRYGzM1LDCn"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dQklrxgLDCp",
        "outputId": "77fb7698-4561-45a7-d2f4-8331bf4b4f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique characters :  27\n",
            "txt_data_size :  81\n"
          ]
        }
      ],
      "source": [
        "# load text data\n",
        "\n",
        "txt_data = \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz \" # input data\n",
        "# txt_data = \"I had neither kith nor kin in England, and was therefore as free as air. I had neither kith nor kin in England, and was therefore as free as air.\"\n",
        "# txt_data = open('input.txt', 'r').read() # test external files\n",
        "\n",
        "chars = list(set(txt_data)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(txt_data)\n",
        "\n",
        "print(\"unique characters : \", num_chars) # You can see the number of unique characters in your input data.\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUDwvNF2LDCv"
      },
      "source": [
        "# One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gceJWR1wLDCw",
        "outputId": "f49ad7dd-4e59-41e5-ca24-24d62605b3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'f': 0, 'j': 1, 'w': 2, 'k': 3, 'q': 4, 'l': 5, 'p': 6, 'g': 7, 'v': 8, 't': 9, ' ': 10, 'r': 11, 'n': 12, 'e': 13, 'z': 14, 's': 15, 'x': 16, 'y': 17, 'm': 18, 'u': 19, 'o': 20, 'b': 21, 'd': 22, 'i': 23, 'c': 24, 'h': 25, 'a': 26}\n",
            "----------------------------------------------------\n",
            "{0: 'f', 1: 'j', 2: 'w', 3: 'k', 4: 'q', 5: 'l', 6: 'p', 7: 'g', 8: 'v', 9: 't', 10: ' ', 11: 'r', 12: 'n', 13: 'e', 14: 'z', 15: 's', 16: 'x', 17: 'y', 18: 'm', 19: 'u', 20: 'o', 21: 'b', 22: 'd', 23: 'i', 24: 'c', 25: 'h', 26: 'a'}\n",
            "----------------------------------------------------\n",
            "[26, 21, 24, 22, 13, 0, 7, 25, 23, 1, 3, 5, 18, 12, 20, 6, 4, 11, 15, 9, 19, 8, 2, 16, 17, 14, 10, 26, 21, 24, 22, 13, 0, 7, 25, 23, 1, 3, 5, 18, 12, 20, 6, 4, 11, 15, 9, 19, 8, 2, 16, 17, 14, 10, 26, 21, 24, 22, 13, 0, 7, 25, 23, 1, 3, 5, 18, 12, 20, 6, 4, 11, 15, 9, 19, 8, 2, 16, 17, 14, 10]\n",
            "----------------------------------------------------\n",
            "data length :  81\n"
          ]
        }
      ],
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in txt_data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o40GEOv2LDCz",
        "outputId": "db0de0f7-f5de-4463-ced0-ca58c9e99aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(81, 27)\n",
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "a\n"
          ]
        }
      ],
      "source": [
        "# Not actually used.\n",
        "\n",
        "onehot_encoded = []\n",
        "\n",
        "for ix in integer_encoded: # ix is an index mapped to a unique character.\n",
        "    letter = [0 for _ in range(len(chars))] # A list len is equal to the number of unique characters and whose elements are all zero.\n",
        "    letter[ix] = 1 # 'letter' is a one-hot vector.\n",
        "    onehot_encoded.append(letter) # Add a 1d list(a vector for one character).\n",
        "onehot_encoded = np.array(onehot_encoded) # list to np-array\n",
        "\n",
        "print(onehot_encoded.shape)     #  = (len(data),len(chars))\n",
        "print(onehot_encoded)\n",
        "\n",
        "# invert encoding\n",
        "inverted = int_to_char[np.argmax(onehot_encoded[0])] # \"argmax\" returns the index of the largest value.\n",
        "print(inverted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtAsJmHVLDC0"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KeTLRbGLDC1"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 10000\n",
        "sequence_length = 10\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 100  # size of hidden layer of neurons.\n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden.\n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxzafCNZLDC1"
      },
      "source": [
        "# Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pZX_zLLDC2"
      },
      "outputs": [],
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "\n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "\n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).\n",
        "\n",
        "        xs[t] = np.zeros((num_chars,1))\n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state.\n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars.\n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        "\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1))\n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)\n",
        "\n",
        "    return loss, ps, hs, xs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maSU_7cfLDC3"
      },
      "source": [
        "# Backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYC2XFhKLDC4"
      },
      "outputs": [],
      "source": [
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1)\n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h.\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.\n",
        "\n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwFKAnFPLDC5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpDB6er5LDC5",
        "outputId": "48b3e8ad-7e00-4e36-fec6-904036cb5783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss: 2.537216\n",
            "iter 100, loss: 0.015741\n",
            "iter 200, loss: 0.006671\n",
            "iter 300, loss: 0.004121\n",
            "iter 400, loss: 0.002949\n",
            "iter 500, loss: 0.002286\n",
            "iter 600, loss: 0.001861\n",
            "iter 700, loss: 0.001566\n",
            "iter 800, loss: 0.001351\n",
            "iter 900, loss: 0.001186\n",
            "iter 1000, loss: 0.001057\n",
            "iter 1100, loss: 0.000952\n",
            "iter 1200, loss: 0.000866\n",
            "iter 1300, loss: 0.000794\n",
            "iter 1400, loss: 0.000732\n",
            "iter 1500, loss: 0.000680\n",
            "iter 1600, loss: 0.000634\n",
            "iter 1700, loss: 0.000594\n",
            "iter 1800, loss: 0.000558\n",
            "iter 1900, loss: 0.000527\n",
            "iter 2000, loss: 0.000499\n",
            "iter 2100, loss: 0.000473\n",
            "iter 2200, loss: 0.000450\n",
            "iter 2300, loss: 0.000429\n",
            "iter 2400, loss: 0.000410\n",
            "iter 2500, loss: 0.000392\n",
            "iter 2600, loss: 0.000376\n",
            "iter 2700, loss: 0.000361\n",
            "iter 2800, loss: 0.000347\n",
            "iter 2900, loss: 0.000334\n",
            "iter 3000, loss: 0.000322\n",
            "iter 3100, loss: 0.000311\n",
            "iter 3200, loss: 0.000301\n",
            "iter 3300, loss: 0.000291\n",
            "iter 3400, loss: 0.000282\n",
            "iter 3500, loss: 0.000273\n",
            "iter 3600, loss: 0.000265\n",
            "iter 3700, loss: 0.000257\n",
            "iter 3800, loss: 0.000250\n",
            "iter 3900, loss: 0.000243\n",
            "iter 4000, loss: 0.000237\n",
            "iter 4100, loss: 0.000230\n",
            "iter 4200, loss: 0.000224\n",
            "iter 4300, loss: 0.000219\n",
            "iter 4400, loss: 0.000213\n",
            "iter 4500, loss: 0.000208\n",
            "iter 4600, loss: 0.000204\n",
            "iter 4700, loss: 0.000199\n",
            "iter 4800, loss: 0.000194\n",
            "iter 4900, loss: 0.000190\n",
            "iter 5000, loss: 0.000186\n",
            "iter 5100, loss: 0.000182\n",
            "iter 5200, loss: 0.000178\n",
            "iter 5300, loss: 0.000175\n",
            "iter 5400, loss: 0.000171\n",
            "iter 5500, loss: 0.000168\n",
            "iter 5600, loss: 0.000165\n",
            "iter 5700, loss: 0.000162\n",
            "iter 5800, loss: 0.000159\n",
            "iter 5900, loss: 0.000156\n",
            "iter 6000, loss: 0.000153\n",
            "iter 6100, loss: 0.000150\n",
            "iter 6200, loss: 0.000148\n",
            "iter 6300, loss: 0.000145\n",
            "iter 6400, loss: 0.000143\n",
            "iter 6500, loss: 0.000140\n",
            "iter 6600, loss: 0.000138\n",
            "iter 6700, loss: 0.000136\n",
            "iter 6800, loss: 0.000134\n",
            "iter 6900, loss: 0.000131\n",
            "iter 7000, loss: 0.000129\n",
            "iter 7100, loss: 0.000128\n",
            "iter 7200, loss: 0.000126\n",
            "iter 7300, loss: 0.000124\n",
            "iter 7400, loss: 0.000122\n",
            "iter 7500, loss: 0.000120\n",
            "iter 7600, loss: 0.000118\n",
            "iter 7700, loss: 0.000117\n",
            "iter 7800, loss: 0.000115\n",
            "iter 7900, loss: 0.000114\n",
            "iter 8000, loss: 0.000112\n",
            "iter 8100, loss: 0.000111\n",
            "iter 8200, loss: 0.000109\n",
            "iter 8300, loss: 0.000108\n",
            "iter 8400, loss: 0.000106\n",
            "iter 8500, loss: 0.000105\n",
            "iter 8600, loss: 0.000104\n",
            "iter 8700, loss: 0.000102\n",
            "iter 8800, loss: 0.000101\n",
            "iter 8900, loss: 0.000100\n",
            "iter 9000, loss: 0.000099\n",
            "iter 9100, loss: 0.000098\n",
            "iter 9200, loss: 0.000096\n",
            "iter 9300, loss: 0.000095\n",
            "iter 9400, loss: 0.000094\n",
            "iter 9500, loss: 0.000093\n",
            "iter 9600, loss: 0.000092\n",
            "iter 9700, loss: 0.000091\n",
            "iter 9800, loss: 0.000090\n",
            "iter 9900, loss: 0.000089\n"
          ]
        }
      ],
      "source": [
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "\n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "\n",
        "    for b in range(batch_size):\n",
        "\n",
        "        inputs = [char_to_int[ch] for ch in txt_data[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in txt_data[data_pointer+1:data_pointer+sequence_length+1]] # t+1\n",
        "\n",
        "        if (data_pointer+sequence_length+1 >= len(txt_data) and b == batch_size-1): # processing of the last part of the input data.\n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "\n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs)\n",
        "\n",
        "\n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y],\n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "        data_pointer += sequence_length # move data pointer\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEHk9ZJgLDC7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OBvuEZqLDC8"
      },
      "outputs": [],
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1))\n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y))\n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1\n",
        "        ixes.append(ix) # list\n",
        "    txt = ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1deWoO5GLDC8",
        "outputId": "51dce896-3718-4314-e86f-541908ca1b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            " bcdefghijklmnopqrstuvwxyz yuvw \n",
            "----\n"
          ]
        }
      ],
      "source": [
        "predict('a',30) # (char, len of output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcn8jtEcLDC-",
        "outputId": "cf479e3a-c75f-495f-f6fc-042c138b6216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            " cdefghijklmnopqrstuvwxyz abcde \n",
            "----\n"
          ]
        }
      ],
      "source": [
        "predict('b',30)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}