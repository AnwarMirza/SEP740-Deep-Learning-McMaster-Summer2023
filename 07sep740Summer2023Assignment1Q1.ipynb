{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUyWCZGn9sBeroklMgeLzW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnwarMirza/SEP740-Deep-Learning-McMaster-Summer2023/blob/main/07sep740Summer2023Assignment1Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEP740 Deep Learning \\\n",
        "Summer 2023 \\\n",
        "# <font color=\"red\" size=\"18pt\">Assignment 1 </font>\n",
        "## Question 1"
      ],
      "metadata": {
        "id": "K5ZhAdcc6h3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fztvy9JW6c3C"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits import mplot3d  # for 3d plottng, using mpl (MatPlotLib) toolkits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize random number generator\n",
        "rng = np.random.default_rng(10)"
      ],
      "metadata": {
        "id": "_F41GbHh67Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "# Activation function\n",
        "def activationFunc(x):\n",
        "  # linear activation function\n",
        "  return (x)\n",
        "\n",
        "# Derivative of the activation function\n",
        "def dActivationFunc(fx):\n",
        "  # derivative of the bipolar sigmoid\n",
        "  return (np.ones(fx.shape))\n",
        "\n",
        "# Feedforward function\n",
        "def feedforward(x, b1, w1, b2, w2):\n",
        "  # weighted sum of inputs reaching at the hidden units\n",
        "  # print('b1 ', b1)\n",
        "  # print('np.dot(',w1, ', ', x, ')  = ', np.dot(w1, x))\n",
        "  z1 = b1 + np.dot(w1, x)\n",
        "  # print('z1 = b1 + np.dot(w1, x) = ', z1)\n",
        "  # output of the hidden units (applying activation function)\n",
        "  a1 = activationFunc(z1)\n",
        "  da1 = dActivationFunc(a1)\n",
        "\n",
        "  # weighted sum of inputs reaching at the output units\n",
        "  # print('b2 ', b2)\n",
        "  # print('np.dot(',w2, ', ', a1, ')  = ', np.dot(w2, a1))\n",
        "  z2 = b2 + np.dot(w2,a1)\n",
        "  # print('z2 = b2 + np.dot(w2,a1) = ', z2)\n",
        "  # output of the output units (applying activation function)\n",
        "  a2 = activationFunc(z2)\n",
        "  da2 = dActivationFunc(a2)\n",
        "  return (z1, a1, da1, z2, a2, da2)\n",
        "\n",
        "# Backpropagation function\n",
        "def backprop(learningRate, xtrain, ytrain, b1, w1, b2, w2):\n",
        "\n",
        "  N = ytrain.shape[0]\n",
        "\n",
        "  # feedforward the signal\n",
        "  z1, a1, da1, z2, a2, da2 = feedforward(xtrain, b1, w1, b2, w2)\n",
        "\n",
        "  # calculate error\n",
        "  err = msError(a2, ytrain)\n",
        "  print('err = ', err)\n",
        "\n",
        "  # error at the output layer\n",
        "  dif = a2 - ytrain\n",
        "  print('dif = ', dif)\n",
        "\n",
        "  # local gradients (deltas) at the output units\n",
        "  deltaOut = dif  * da2\n",
        "  deltaOut = deltaOut.reshape(deltaOut.shape[0],1)\n",
        "  print('deltaOut.shape = ', deltaOut.shape)\n",
        "  print('deltaOut = ', deltaOut)\n",
        "  a1 = a1.reshape(a1.shape[0],1)\n",
        "  print('a1.shape = ', a1.shape)\n",
        "  print('a1 = ', a1)\n",
        "  # calculate weights and bias changes\n",
        "  print('learningRate ', learningRate)\n",
        "  print(' deltaOut = ',deltaOut, ', a1.T = ', a1.T, ' N = ',N)\n",
        "  # print(-learningRate, ' * np.dot(',deltaOut, ', ', a1.T, ')/',N,'  = ', -learningRate * np.dot(deltaOut,a1.T)/N)\n",
        "  deltaW2 = -learningRate * np.dot(a1.T, deltaOut)/N\n",
        "  deltaB2 = -learningRate * deltaOut/N\n",
        "\n",
        "  # local gradients (deltas) at the hidden units\n",
        "  deltaInHid = np.dot(w2.T, deltaOut)\n",
        "  deltaHid = deltaInHid * da1\n",
        "  # calculate weights and bias changes\n",
        "  deltaW1 = -learningRate * np.dot(deltaHid, xtrain.T)/N\n",
        "  deltaB1 = -learningRate * deltaHid/N\n",
        "\n",
        "  return (deltaB1, deltaW1, deltaB2, deltaW2, err)\n",
        "\n",
        "# Mean squared error function\n",
        "def msError(yPredicted, yTrue):\n",
        "  diff = yPredicted - yTrue\n",
        "  error = np.dot(diff.T, diff) /(2*yTrue.shape[0])\n",
        "  return (error)"
      ],
      "metadata": {
        "id": "SMLHNBT168J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training data\n",
        "sample = np.array([0.9, 0.1])\n",
        "target = np.array([0.9])\n",
        "\n",
        "print('sample.shape = ', sample.shape)\n",
        "print('target.shape = ', target.shape)\n",
        "print([sample, target])\n",
        "print('\\n\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyrtpupW7QLZ",
        "outputId": "ba5de127-244f-40dc-8ef5-e80b499cc572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample.shape =  (2,)\n",
            "target.shape =  (1,)\n",
            "[array([0.9, 0.1]), array([0.9])]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Parameters - these will stay fixed for both parts (a) and (b)\n",
        "\n",
        "# Initialize network parameters\n",
        "learningRate = 0.2\n",
        "epsilon = 0.01\n",
        "Ni = 2  # Ni input neurons\n",
        "Nh = 2   # Nh hidden neurons\n",
        "No = 1   # No output neurons\n",
        "\n",
        "# Initialize weights and bias (random numbers between -1 and +1)\n",
        "# x = a + (b-a) * rng.random()\n",
        "v0 = np.array([-0.4, 0.7])\n",
        "v = np.array([[-0.2, -0.7],[0.2, 0.1]])\n",
        "w0 = np.array([0.1])\n",
        "w = np.array([-0.3, -0.6])\n",
        "\n",
        "\n",
        "# b1_init = -1 + 2 * rng.random((Nh,1))\n",
        "# w1_init = -1 + 2 * rng.random((Nh, Ni))\n",
        "# b2_init = -1 + 2 * rng.random((No,1))\n",
        "# w2_init = -1 + 2 * rng.random((No, Nh))"
      ],
      "metadata": {
        "id": "KJYMBGjV7AXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward the signal\n",
        "z1, a1, da1, z2, a2, da2 = feedforward(sample, v0, v, w0, w)\n",
        "print(z1, a1, da1, z2, a2, da2)\n",
        "\n",
        "N = target.shape[0]\n",
        "\n",
        "# calculate error\n",
        "err = msError(a2, target)\n",
        "print('err = ', err)\n",
        "\n",
        "# error at the output layer\n",
        "dif = a2 - target\n",
        "print('dif = ', dif)\n",
        "\n",
        "# local gradients (deltas) at the output units\n",
        "deltaOut = dif  * da2\n",
        "deltaOut = deltaOut.reshape(deltaOut.shape[0],1)\n",
        "print('deltaOut.shape = ', deltaOut.shape)\n",
        "print('deltaOut = ', deltaOut)\n",
        "a1 = a1.reshape(a1.shape[0],1)\n",
        "print('a1.shape = ', a1.shape)\n",
        "print('a1 = ', a1)\n",
        "# calculate weights and bias changes\n",
        "print('learningRate ', learningRate)\n",
        "# print(' deltaOut = ',deltaOut, ', a1.T = ', a1.T, ' N = ',N)\n",
        "print(-learningRate, ' * np.dot(',a1, ', ', deltaOut, ')/',N,'  = ', -learningRate * np.dot(a1,deltaOut)/N)\n",
        "deltaW2 = -learningRate * np.dot(a1, deltaOut)/N\n",
        "deltaB2 = -learningRate * deltaOut/N\n",
        "print('deltaW2 ', deltaW2)\n",
        "print('deltaB2 ', deltaB2)\n",
        "\n",
        "\n",
        "\n",
        "# local gradients (deltas) at the hidden units\n",
        "deltaInHid = np.dot(w2.T, deltaOut)\n",
        "deltaHid = deltaInHid * da1\n",
        "# calculate weights and bias changes\n",
        "deltaW1 = -learningRate * np.dot(deltaHid,sample)/N\n",
        "deltaB1 = -learningRate * deltaHid/N\n",
        "print('deltaW1 ', deltaW1)\n",
        "print('deltaB1 ', deltaB1)\n",
        "\n",
        "\n",
        "\n",
        "# # Backpropagation of errors\n",
        "# deltaV0, deltaV, deltaW0, deltaW, err = backprop(learningRate, sample, target, v0, v, w0, w)\n",
        "# # print('deltaV0, deltaV, deltaW0, deltaW, err', deltaV0, deltaV, deltaW0, deltaW, err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "sBcCItCX9Hyk",
        "outputId": "d8bc5b19-3311-4800-be30-2bbe0d6b949f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.65  0.89] [-0.65  0.89] [1. 1.] [-0.239] [-0.239] [1.]\n",
            "err =  0.6486605\n",
            "dif =  [-1.139]\n",
            "deltaOut.shape =  (1, 1)\n",
            "deltaOut =  [[-1.139]]\n",
            "a1.shape =  (2, 1)\n",
            "a1 =  [[-0.65]\n",
            " [ 0.89]]\n",
            "learningRate  0.2\n",
            "-0.2  * np.dot( [[-0.65]\n",
            " [ 0.89]] ,  [[-1.139]] )/ 1   =  [[-0.14807 ]\n",
            " [ 0.202742]]\n",
            "deltaW2  [[-0.14807 ]\n",
            " [ 0.202742]]\n",
            "deltaB2  [[0.2278]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-4161ba0a6e73>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# local gradients (deltas) at the hidden units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdeltaInHid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltaOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdeltaHid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltaInHid\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mda1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# calculate weights and bias changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'w2' is not defined"
          ]
        }
      ]
    }
  ]
}